{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import  gym\n",
    "import numpy as np\n",
    "import time\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, s, na):\n",
    "    epsilon = 0.3\n",
    "    p = np.random.uniform(low=0, high=1)\n",
    "    \n",
    "    if p > epsilon:\n",
    "        return np.argmax(Q[s,:])\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes about FrozenLake\n",
    "https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "Environment is described as a grid of characters e.g\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "\n",
    "S: starting point\n",
    "F: Frozen lake, safe to navigate but there is a chance that the agent will slide and go in the wrong direction\n",
    "H: hole - agent dies if it falls here\n",
    "G: goal - where the agent wants to end up to win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-10-13 20:16:35,085] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('initial state: ', 0)\n",
      "()\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "('Number of actions: ', 4)\n",
      "('Number of states: ', 16)\n"
     ]
    }
   ],
   "source": [
    "#Setup environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "s = env.reset()\n",
    "print(\"initial state: \" , s)\n",
    "print()\n",
    "\n",
    "env.render()\n",
    "\n",
    "# Other environments can have different shapes for the observation space e.g (the screen or RAM)\n",
    "# which has very interesting implications in AI research\n",
    "# However, if you're making AI for a video game and you can control the action and observation space yourself\n",
    "# this technique will work just fine\n",
    "print(\"Number of actions: \", env.action_space.n )\n",
    "print(\"Number of states: \", env.observation_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q learning\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "lr = 0.5 # how quickly you learn\n",
    "y = 0.9 # discount factor\n",
    "eps = 10000 # total number of iterations\n",
    "\n",
    "for i in range(eps):\n",
    "    s = env.reset()\n",
    "    t = False\n",
    "    while True:\n",
    "        a = epsilon_greedy(Q, s, env.action_space.n)\n",
    "        # s = state the agent is in\n",
    "        # r = the reward the agent received\n",
    "        # t = whether the game is over or not\n",
    "        s_, r, t, _ = env.step(a)\n",
    "        if(r == 0):\n",
    "            if t == True:\n",
    "                r = -5\n",
    "                Q[s_] = np.ones(env.action_space.n) * r\n",
    "            else:\n",
    "                r = -1 #punish long routes\n",
    "        if(r==1):\n",
    "            r = 100 #found the goal\n",
    "            Q[s_] = np.ones(env.action_space.n) * r\n",
    "        \n",
    "        # Weighted average between the old estimate and new estimate of a state action pair\n",
    "        Q[s,a] = Q[s,a] + lr * (r + y * np.max(Q[s_,a]) - Q[s,a])\n",
    "        s = s_\n",
    "        if(t == True):\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -9.852764     -9.68111881   -9.67810808   -9.99999995]\n",
      " [  -9.72523117   -9.68251325   -9.65632413   -9.99999995]\n",
      " [  -9.61961312   -9.64961979   -9.640255     -9.99999994]\n",
      " [  -9.6338988    -9.52499786   -9.61882611   -9.99999992]\n",
      " [  -9.79882065   -9.53937083   -9.66187623   -9.94932264]\n",
      " [  -5.           -5.           -5.           -5.        ]\n",
      " [  -9.62758867   -9.2106401    -9.33185493   -9.71877684]\n",
      " [  -5.           -5.           -5.           -5.        ]\n",
      " [  -9.67057912   -9.49487724   -9.56714894   -9.79917382]\n",
      " [  -9.60187359   -8.0280717    -8.5707085    -9.60310979]\n",
      " [  -9.71266298   47.72740638   -5.56933513   -9.63672357]\n",
      " [  -5.           -5.           -5.           -5.        ]\n",
      " [  -5.           -5.           -5.           -5.        ]\n",
      " [  -9.60897513   -8.55934243   -4.60149797   -8.04984241]\n",
      " [  -9.71820133   51.05729801    1.14013016   -6.47730933]\n",
      " [ 100.          100.          100.          100.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "============\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "============\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "============\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "============\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "============\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "============\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "============\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "============\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "============\n",
      "SFFF\n",
      "FHFH\n",
      "FFF\u001b[41mH\u001b[0m\n",
      "HFFG\n",
      "  (Down)\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "env.render()\n",
    "while(True):\n",
    "    a = np.argmax(Q[s])\n",
    "    s_,r,t,_ = env.step(a)\n",
    "    print(\"============\")\n",
    "    env.render()\n",
    "    s = s_\n",
    "    if(t==True):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q learning\n",
    "Your game probably has a much larger action and observation space than FrozenLake\n",
    "Main idea of deep Q learning is to approximate the Q matrix using a neural network which is why we'll be looking at it\n",
    "in the next tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
